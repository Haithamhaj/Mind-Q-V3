# Mind‑Q V3 — وثيقة المنطق التشغيلي للمراحل (بدون كود)

هذه الورقة تشرح كيف يعمل النظام خطوة بخطوة عبر المراحل، وما هي القواعد والخوارزميات والمعادلات المستخدمة لاتخاذ القرارات. الهدف: فهم المشروع ومنطقه بعيدًا عن تفاصيل التنفيذ البرمجية. للاستزادة حول الهيكل العام راجع `docs/architecture.md` و`docs/phases.md`.

## مبادئ عامة

- كل مرحلة مستقلة بمنطقها تحت `backend/app/services/` وتنتج مخرجات وسيطة محفوظة في `backend/artifacts/` لسهولة التتبع والعرض.
- القرارات مبنية على قواعد واضحة مع حدود تحذير/إيقاف لمنع نتائج غير سليمة.
- لا تُستخدم قاعدة بيانات في الـMVP؛ الاعتماد على ملفات Parquet/JSON داخل مجلد `artifacts`.

## خريطة المراحل باختصار

انظر الجدول في `docs/phases.md`. أدناه شرح أعمق لمنطق كل مرحلة وما تنتجه.

---

## Phase 0 — Quality Control (تأسيس)

- فحوصات صحّة البيئة وبنية المشروع وتشغيل الخدمات. الهدف ضمان أساس ثابت قبل أي معالجة بيانات.

## Phase 1 — Goal & KPIs (الأهداف والمؤشرات)

- التقاط أهداف العمل والمؤشرات المطلوبة لاحقًا للتقييم. لا يوجد حسابات إلزامية هنا؛ إنما توصيف متطلبات القياس.

## Phase 2 — Ingestion & Landing (الاستيعاب)

- قراءة المصدر (CSV/Excel/Parquet/JSON) مع إستراتيجيات تعافٍ لملفات CSV المتضررة.
- تنقية أسماء الأعمدة: تحويل لصيغة قياسية (أحرف صغيرة/شرطة سفلية/إزالة الرموز).
- الكتابة إلى Parquet مع ضغط Zstd، وإخراج نسبة الضغط ووقت الاستيعاب.
- قاعدة التقسيم على دفعات: إذا كان الحجم > 1GB يستخدم «التجزئة» بدل كتابة دفعة واحدة.
- مخرجات نموذجية: `raw_ingested.parquet`، مقاييس الحجم والزمن ونسبة الضغط.

معادلة نسبة الضغط التقريبية: $\text{ratio} = \frac{\text{source\_size}}{\text{parquet\_size}}$.

## Phase 3 — Schema & Dtypes (المخطط وأنواع البيانات)

- استدلال الأنواع والتحويل:
  - أعمدة الهوية → `string`، التواريخ → `datetime[UTC]`، الرقمي → `float`، المنخفض التباين النصي → `category`.
  - محاولات تحويل ذكية للأرقام (مثل "%")، وتحديد التواريخ بأساليب اسم/نمط.
- تصنيف الأعمدة إلى: معرفات/تواريخ/رقمية/فئوية + توليد مخطط Pandera على شكل JSON.
- حساب معدل الانتهاكات وإصدار حالة: إذا $\text{violation\_rate} > 2\%$ ⇒ WARN.

## Phase 4 — Profiling (التوصيف الإحصائي)

- استخدام كامل البيانات لزيادة دقة التحضير لـML.
- للرقمية: المتوسط، الوسيط، الانحراف، رباعيات، التواء (Skewness) وتفلطح (Kurtosis).
- للفئوية: عدد القيم المميزة، الأكثر تكرارًا، أفضل 5 فئات.
- المفقودات: عدد ونسبة لكل عمود.
- اكتشاف القيم الشاذة IQR بقاعدة قوية: حد سفلي/علوي $= Q1 \pm 3\,IQR$.
- معاينة ارتباطات عددية أعلى 5 أزواج.
- تُخزَّن أهم 10 قضايا جودة في `artifacts/dq_report.json`.

تعريفات مختصرة:
- $\text{skew}(x)$، $\text{kurt}(x)$ بالطريقة الاعتيادية (SciPy).
- $\text{IQR} = Q3 - Q1$.

## Phase 5 — Missing Data (التعامل مع القيم المفقودة)

- شجرة قرار على مستوى العمود (اختصارًا):
  1) التواريخ: عدم الإقحام، فقط إضافة علم `missing`.
  2) رقمي قليل الفقد ($\le 5\%$): وسيط.
  3) وجود عمود مجموعات `group_col` وفقد حتى 50%: وسيط جماعي Group Median.
  4) بيانات ضخمة ($n \ge 50{,}000$) مع ≥3 ميزات مترابطة (|r|≥0.4): MICE.
  5) صغيرة جدًا ($n<2000$): تجنب KNN/MICE، استخدم وسيط (أو وسيط جماعي إن توفر).
  6) افتراضيًا: KNN (k=5) للأرقام.
  7) الفئوية: نمط/نمط جماعي، أو تعيين "Unknown".
- بعد الإقحام: تحقق جودة بالتوزيعات عبر PSI وKS.
- تقييم الحالة: PASS/WARN/STOP حسب اكتمال السجلات ونسبة فشل فحوصات التحقق.

معادلات التحقق:
- PSI مع تقسيم إلى حاويات مشتركة: $$\text{PSI} = \sum_{b} (p_b - q_b)\,\ln\left(\frac{p_b}{q_b}\right)$$ حيث $p_b$ توزيع الأصل و$q_b$ بعد الإقحام. حدود مقترحة: تحذير عند $\approx 0.10$.
- إحصاء كولموغوروف–سميرنوف: $$D = \sup_x |F_{\text{orig}}(x) - F_{\text{imp}}(x)|$$ ويُفضَّل $D \le 0.10$.

## Phase 6 — Standardization (التوحيد)

- تطبيع Unicode للنصوص + معالجات عربية: توحيد الألف/الياء/التاء المربوطة.
- تطبيق خرائط معيارية حسب المجال (لوجستيات/صحة/تجزئة ...). مثال: توحيد أسماء شركات الشحن وحالات الشحنة.
- طي الفئات النادرة إلى "Other" وفق حد تكرار. في البيانات الصغيرة (≤1000 صف) يفرض حد فعال لا يقل عن 3% لتجنب فئات ضئيلة غير مفيدة.

## Phase 7 — Feature Draft (اشتقاق الميزات)

- اشتقاقات نوعية بالمجال:
  - لوجستيات: `transit_time` بالساعات، علم SLA ≤ 48 ساعة، علم RTO.
  - صحّة: `los_days` (مدة الإقامة)، `age_group` فئات عمرية.
  - تجزئة/تسويق/تمويل: اشتقاقات مناسبة (قيمة الطلب، CTR، مدة القرض...).
- وضع سقف للقيم المتطرفة على بعض الميزات المشتقة حسب المجال (مثلاً `transit_time` ≤ 240 ساعة).

## Phase 7.5 — Encoding & Scaling (الترميز والمقاييس)

- الترميز للفئيات على مجموعة التدريب فقط:
  - عدد فئات ≤ 50 ⇒ One‑Hot.
  - عدد فئات > 50 وبيانات ضخمة ($n>50{,}000$) ومع هدف متاح ⇒ Target Encoding.
  - خلاف ذلك ⇒ Ordinal بسيط.
- القياس للرقمية على التدريب فقط ثم تطبيق التحويل نفسه على التحقق/الاختبار:
  - افتراضي: StandardScaler.
  - مجال التمويل: RobustScaler لمقاومة الذيول الثقيلة.
- تُحفَظ النماذج/المقاييس في `artifacts/` (مثل `scaler_numeric.joblib`).

الزّيـ-تقييس: $$z = \frac{x - \mu}{\sigma}$$

## Phase 8 — Merging & Keys (الدمج والمفاتيح)

- اختيار مفتاح مشترك تلقائيًا (أعمدة تحوي "id").
- سياسة التكرارات:
  - نسبة التكرار > 10% ⇒ إيقاف.
  - بين 3% و10% ⇒ الاحتفاظ بأحدث سجل (يفضَّل اعتماد طابع زمني).
  - < 3% ⇒ توثيق فقط.
- سياسة الأيتام بعد الدمج (قيَم ملتحقة مفقودة):
  - > 10% ⇒ إيقاف.
  - 2%–10% ⇒ عزل الأيتام وحفظهم ثم الاستمرار.
  - < 2% ⇒ تنبيه فقط.
- تُحفَظ جداول الأيتام عند الحاجة في `artifacts/`.

## Phase 9 — Correlations & Associations (الارتباطات)

- للرقمية: معامل بيرسون مع قيمة p.
- للفئوية: جدولة طوارئ، اختبار كاي‑تربيع، ثم Cramér’s V.
- تنظيف أنواع البيانات وإسقاط الحالات غير الصالحة زوجيًا.
- عند كثرة الاختبارات (>20) تُطبَّق تصحيحات تعدد المقارنات: FDR‑BH، أو Bonferroni عند عدم توفر الحزمة.

معادلات:
- بيرسون: $$r = \frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum (x_i-\bar{x})^2}\;\sqrt{\sum (y_i-\bar{y})^2}}$$
- Cramér’s V: $$V = \sqrt{\frac{\chi^2}{n\,(\min(r,c)-1)}}$$ مع $\chi^2$ من كاي‑تربيع، و$r,c$ أبعاد الجدول.

## Phase 9.5 — Business Validation (تحقق منطق الأعمال)

- مصفوفة علاقات متوقعة لكل مجال (إيجابي/سلبي/بدون/غير واضح).
- تُقارن الارتباطات المرصودة بالتوقعات وتُصنَّف المخالفة: عالية/متوسطة/منخفضة.
- تُنشأ فرضيات تفسيرية نصية تلقائيًا للحالات غير المتوقعة (LLM) لمراجعة الخبراء.
- حالة عامة: أكثر من مخالفتين شديدتين ⇒ STOP؛ خلاف ذلك WARN/ PASS.

## Phase 10 — Packaging (تجهيز الحِزمة)

- تجميع القطع الأساسية (تقارير الجودة/الملفات المعيارية/المخططات/البيانات المدمجة)، وإنشاء `provenance.json` و`changelog.md` ثم حزمة ZIP `eda_bundle.zip`.
- إنشاء بصمة تتبُّع (SHA‑256 مختصرة) لسلامة المخرجات.

## Phase 10.5 — Train/Val/Test Split (التقسيم)

- خياران:
  - زمني: ترتيب بالزمن دون خلط.
  - طبقي: الحفاظ على توزيع الهدف عبر المجموعات إن وجد هدف.
- نسب افتراضية: Train ≈ $(1-\text{test}-\text{val})$، Val ≈ $\frac{\text{val}}{1-\text{test}}$ من مجموعة train+val.
- إخراج توزيع الهدف لكل مجموعة للتدقيق.

## Phase 11 — Advanced Exploration (استكشاف متقدم)

- التجميع K‑Means مع بحث عن $k$ الأمثل ضمن نطاق معقول وقياس Silhouette.
- تحليل مركبات رئيسية PCA للاحتفاظ بـ ≥90% من التباين عند كثرة الميزات.
- كشف الشذوذ Isolation Forest بنسبة تلوث 5% وتسجيل علم الشذوذ.

Silhouette: $$s = \frac{b - a}{\max(a,b)}$$ حيث $a$ متوسط التباعد داخل العنقود و$b$ الأدنى خارج العنقود.

## Phase 11.5 — Feature Selection (اختيار الميزات)

- ترتيب قائم على نموذج غابة عشوائية (تصنيف/انحدار حسب طبيعة الهدف).
- RFE بواسطة انحدار لوجستي لاختيار عدد مستهدف من الميزات.
- دمج القائمتين وإدراج ميزات "معتمدة أعماليًا" قسريًا ثم حصرها في أعلى `top_k`.
- فحص التضخيم التبايني VIF بهدف < 5.

VIF: $$\text{VIF}(x_j) = \frac{1}{1 - R_j^2}$$ حيث $R_j^2$ من إرجاع $x_j$ على بقية الميزات.

## Phase 12 — Text Analysis (تحليل النص)

- رصد أعمدة النص: متوسط طول > 50 حرفًا.
- كشف اللغة (عربي/إنجليزي/مختلط) عبر نسبة المحارف العربية في عينة.
- توصية المعالجة:
  - بيانات > 500k صف: تجاوز التحليل.
  - حجم نص > 50MB: خصائص أساسية فقط.
  - غير ذلك: تحليل أساسي + مشاعر (MVP).
- خصائص أساسية: أطوال/كلمات/جمل، نسب الأرقام/الرموز/العربية.

## Phase 13 — Monitoring & Drift (المراقبة والانجراف)

- إعداد خط أساس لكل ميزة رقمية: متوسط/انحراف معياري وحدود للـPSI/KS.
- حدود افتراضية: PSI تحذير 0.10 وإجراء 0.25؛ KS تحذير 0.10 وإجراء 0.20.
- مخرجات: `drift_config.json` مع قائمة الميزات وختم زمني للخط الأساس.

---

## القطع الأثرية الرئيسية (Artifacts)

- جودة البيانات: `dq_report.json`, `profile_summary.json`.
- سياسات/مواصفات: `imputation_policy.json`, `feature_spec.json`, `mapping_config.json`.
- نتائج التحليلات: `correlations.json`/`correlation_matrix.json`, `business_veto_report.json`.
- البيانات المعالجة: `merged_data.parquet`, `train.parquet`, `validation.parquet`, `test.parquet`، وملفات الترميز/المقاييس.
- الحِزمة: `eda_bundle.zip`, `provenance.json`, `changelog.md`.

## كيف تستخدم هذه الورقة

- لفهم "ما الذي يحدث ولماذا" في كل مرحلة.
- لا تحتوي على كود؛ التنفيذ قد يتغير، لكن المنطق والحدود والمعادلات هنا مرجعية مستقرة.
- للمزيد من التفاصيل التشغيلية والواجهات، راجع: `docs/backend.md`, `docs/validation-and-testing.md`, وملفات الخدمات داخل `backend/app/services/`.
